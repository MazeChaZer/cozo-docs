# Time travel in a database: a Cozo story

You built a social network. The only thing your users could on the network do is to update their "mood". For example, user "joe" may be "moody" yesterday, and "happy" today. How is this reflected in the database? In terms of CozoScript, this can be stored in the following relation:

```
:create status {uid: String => mood: String}
```

The equivalent SQL for Postgres would be

```sql
create table status (
    uid text primary key,
    status text not null
)
```

Your home page is very simple: it is a gigantic page containing the moods of all the users all at once. To generate it, you have the query:

```
?[uid, mood] := *status{uid, mood}
```

```sql
select uid, mood from status
```

And when users want to update their status, you just run:

```
?[uid, mood] <- $input
:put status {uid => mood}
```

```sql
update status set mood = $1 where uid = $2
```

Simple, right? Now scientists came to you and want to buy your data for their study of the fluctuation of moods during the pandemic. Of course you know that their real motive is nefarious, so you promptly showed them the door.
And then started banging your head against the door. Why have you thrown away the _history_, the most valuable part
of your data? WHY?

So you borrowed a time machine from a future you and travelled back in time 
to warn the former you. "It's simple enough to fix", the former you said and typed:

```
:create status {uid: String, ts default now() => mood: String}
```

```sql
create table status (
    uid text not null,
    ts timestamptz not null default now(),
    mood text not null,
    primary key (uid, ts)
)
```

where `ts` stands for "timestamp", time elapsed since the UNIX epoch, for example, with a default value equal to the current timestamp if during insertion a value is not provided. Of course there is no way users can delete their
accounts, all they can do is to send you more and more updates. Very sensible!

Now you need to generate your homepage:

```
?[uid, mood] := *status{uid, mood, ts}, ts == now()
```

```sql
select uid, mood from status
where ts = now()
```

Disaster! The homepage remains forever blank no matter what the users do! The problem is that when you generate your homepage, you can only collect data that were inserted in the past. And for past data, the condition `ts == now()` is never true!

After a lot of fumbling, you found that the following query does the trick:

```
candidates[uid, max(ts)] := *status{uid, ts}
?[uid, mood] := candidates[uid, ts], *status{uid, ts, mood}
```

```sql
with candidates(uid, ts) as (
    select uid, max(ts) from status
    group by uid
)
select status.uid, status.mood from status
inner join candidates on status.uid = candidates.uid and status.ts = candidates.ts
```

Let's see what this query does: ...

And when you want to travel back to a particular date:

```
candidates[uid, max(ts)] := *status{uid, ts}, ts < $date_ts
?[uid, mood] := candidates[uid, ts], *status{uid, ts, mood}
```

```sql
with candidates(uid, ts) as (
    select uid, max(ts) from status
    where ts < $1
    group by uid
)
select status.uid, status.mood from status
inner join candidates on status.uid = candidates.uid and status.ts = candidates.ts
where not status.quitted
```

## The cost of time travel

Your social network became a runaway success, not to mention the stash of cash the scientists gave you! 
As time went on, however, you noticed performance problems, and it got worse every day.

What's happening? After all, your network caters only for the students on a certain campus, and even if everyone signed up for it, there would only be 10,000 users at most. After digging into your data, you notice that some (most?) of your users are hyperactive and tend to update their mood every five minutes during their waking hour! Even though you have only run your service for three months, some of these users have already accumulated over 10,000 mood updates (sounds familiar?).

Now look at your query for front-page generation again:

```
candidates[uid, max(ts)] := *status{uid, ts}
?[uid, mood] := candidates[uid, ts], *status{uid, ts, mood, quitted}, !quitted
```

```sql
with candidates(uid, ts) as (
    select uid, max(ts) from status
    group by uid
)
select status.uid, status.mood from status
inner join candidates on status.uid = candidates.uid and status.ts = candidates.ts
where not status.quitted
```

The key point is that you must do a complete scan of your data once to get your results. For 10,000 users and 1,000 updates each (we use the mean number of mood updates, so it's 1,000 instead of 10,000), that's 10 million rows. And next year it will become more than one billion rows, since time ticks and you are thinking of expanding your service to other campuses.

Your investor, who has no doubt became aware of your problem by now, suggest the "enterprise-y" thing: pre-computing the front page and updating it periodically instead of calculating it in real time.

## Dreamy indices

Being a hacker with a big ego, you detest all things "enterprise-y" and asked yourself: "is there anything better that can be done?" Your friend, who works in finance, suggested time series databases. "It can handle data from the _stock market_ quite well, so surely it is good enough for _your_ data!" "Just index your data by the timestamp!" Well, unlike stock market indices, your data is _sparse_: it is not collected in regular intervals for all users in unison. Are you going to materialize these implicit rows so that every time a user updates her mood, _everyone else_ also gets an automatic update with their previous mood? Your cloud provider was very welcoming of this idea and urged you to sign up for their proprietary time series database. Your investor was also kind of supportive since it would make you an instant "big data" company, but worried about whether you could secure additional funding in time to cover the costs. You, ever calm, did some back-of-envelop estimates, and gave up the idea.

Your investor still wanted to talk to you over the phone, but you became very annoyed and went to sleep, clouded in anxiety. In your dream, you came to a dark, Harry-Potteresque library, with dusty shelves neatly lined up, and on the shelves were … books for the mood of your users at different times, meticulously arranged! The _tree_ backing your database had taken physical form in your dream! You walked mechanically to the first shelf, like a robot, and started to collected the mood of every user at midnight some days back.

"Aaron, 2022-09-01, 15:31, too early."

"Aaron, 2022-09-01, 15:33, still too early."

…

"Aaron, 2022-12-24, 23:59, still too early."

"Aaron, 2022-12-25, 00:34, well it's _past_ the time we want, so the _previous_ item contains the mood." (The mood was "festive", by the way.)

"Aaron, 2022-12-25, 00:42, we don't need this anymore."

"Aaron, 2022-12-25, 00:47, we don't need this anymore."

...

"Aaron, 2022-12-27, 12:31, why are we _still_ looking at Aaron, by the way?"

...

"Bean, 2022-11-27, ..."

There were two things that irked you. First, you were going through the data in the wrong direction, so that after you had gone past the expected date, you had to go back and look at the _previous_ record. This was especially annoying since some users signed up only today, and the previous record was someone else's. Second, you were walking a _tree_, so why couldn't you jump to the next user when you knew you were done with a user?

Suddenly, the books poured out of the shelves to form a tornado, swirled all over the library, and after a while returned to the shelves. "I had to do this all over again," you gruntled and walked to the first first shelf. But something had changed: you could now directly _jump_ to the beginning, and the records were in a different order, ascending by the user, as before, but _descending_ by the timestamp:

"Aaron, 2022-12-27, 03:38, too late, let's _jump_ to the book past Aaron, 2022-12-25, 00:00."

"Aaron, 2022-12-24, 23:59. Now _this_ book contains the required mood for Aaron." "Let's now jump to the book past Aaron at the BIG BANG."

"Bean, 2022-12-24, 23:11, this is already the correct book for Bean, lucky! Now let's go past Bean at the BIG BANG." "I wonder what happened to Mr Bean since Christmas Eve?"

…

Suddenly, you woke up. You rushed to your computer and wrote down what you saw, in code.

## Back to the present

Eventually, your social network took over and changed the world, with the simple schema in CozoScript:

```
:create status {uid: String, ts: Validity default 'ASSERT' => mood: String}
```

and the query for the present:

```
?[uid, mood] := *status{uid, mood @ 'NOW'}
```

For historical moments:

```
?[uid, mood] := *status{uid, mood @ '2019-12-31T23:59:59Z'}
```

Obviously, we can no longer give you SQL translations.

Our story ends here. Of course, what you have really read is the story of the new time travel feature in Cozo 0.4.
We have also added a part in the tutorial giving you hands-on experience, and a new chapter in the manual
for more technical discussions about the feature.

## But what about performance?

We did our tests with the same Mac Mini as before: it runs MacOS 13.0.1, has Apple M1 CPUs with 4 performance
cores and 4 efficiency cores, 16GB of memory and a pretty fast NVMe SSD storage.
We only test against the RocksDB backend for simplicity.

We generated many relations, all of them contain data for 10,000 users.
The 'Plain' relation stores no history at all.
The 'Naive' relations store and query history using the naive approach we described first.
We generated different versions of the naive relations, containing different number of
mood updates per user.
Finally, the 'Hopping' relations store and query history using the approach we described in the dream.

The historical timestamp for the queries is chosen randomly and the results are averaged over many runs.

First we want to know how does history affect point query throughput, measured in queries per second (QPS):

|Type|	# updates per user|	QPS|	Performance percentage|
|----|--------------------------|------|---------|
|Plain|	1|	143956| 	100.00%|
|Naive|	1|	106182| 	73.76%|
|Naive|	10|	92335| 	64.14%|
|Naive|	100|	42665| 	29.64%|
|Naive|	1000|	7154| 	4.97%|
|Hopping|	1|	125913| 	87.47%|
|Hopping|	10|	124959| 	86.80%|
|Hopping|	100|	100947| 	70.12%|
|Hopping|	1000|	102193| 	70.99%|

As we can see, for 1000 updates per user, the naive approach has a 20x reduction in
throughput compared to the no history approach. The hopping approach, on the other
hand, maintains more than 70% of the original performance.

To be fair, for the simplest kind of point query where you know the complete key and
the timestamp, and want the result for only a single user, there is a better way
to write the query so that the naive relations can achieve similar performance
to the hopping approach. We deliberately wrote our query in a way to avoid
this optimization, since depending on the query, this optimization cannot always be done.

Next, aggregation results where we need the complete result for all users.
Now we measure latency instead of throughput:

|Type|	# updates per user|	Latency (ms)|	Slowdown ratio|
|----|--------------------------|------|---------|
|Plain|	1|	2.38| 	1.00| 
|Naive|	1|	8.90| 	3.74| 
|Naive|	10|	55.52| 	23.35| 
|Naive|	100|	541.01| 	227.52| 
|Naive|	1000|	5391.75| 	2267.53| 
|Hopping|	1|	9.60| 	4.04| 
|Hopping|	10|	9.99| 	4.20| 
|Hopping|	100|	39.34| 	16.55| 
|Hopping|	1000|	31.41| 	13.21| 

Now the naive approach scales really badly. For a query that takes only 2ms in
a plain relation, it now takes more than 5 seconds in a relation with 1000 historical
facts per user. The hopping approach, on the other hand, keeps everything under control.
You notice that it actually performs better in the relation with 1000 historical facts per user
than in the relation with only 100. This is not a random fluctuation: it occurs
consistently no matter how we test it. Our guess is that the RocksDB backend
did something different when a certain threshold is passed.

Nonetheless, it is important to note that there is at least a 3x slowdown no matter how you store the history, 
even if you only have one historical fact per user. This is the minimal cost of time travel.
And this is why Cozo does not automatically keep history for every relation 
regardless of whether you really need it: our philosophy is "zero-cost and zero-mental-overhead abstraction".