# Time travel in a database: a Cozo story

Simply put, time travel in a database means tracking changes to data over time and allowing queries to be logically executed at a point in time to get a historical view of the data. In a sense, this makes your database "immutable" since nothing is really deleted from the database ever. This is not a new thing: several established databases had it for a long time (but did you see the similarity among them?).

Cozo is a new general-purpose, transactional, relational database that uses Datalog and focuses on graph data and algorithms. The earliest, unreleased version of Cozo DB can time travel, but this capability was later removed since we don't want to pay the significant performance penalty (read until the end to find out more). Now with the release of Cozo 0.4, a much more flexible form of time travel is back, upholding our "zero-cost, zero-mental-overhead abstraction" principle: if you don't use it, you don't pay any performance cost, and you don't have to know anything about it to use the database without the functionality.

In this post we will begin with an story illustrating why time travel is useful in databases, containing a naive implementation that can be done in any relational database. Then we will discuss the shortcomings of the naive implementation and introduce Cozo's simple yet efficient implementation. Finally, we will show some benchmarks so that you know _how much_ you are paying when you opt for the time travel capability (nothing is free).

When appropriate, we will provide SQL translation of the queries, so that you don't need to know CozoScript or Datalog to read this post. By putting the equivalent queries side by side we wish to also convice you the expressivity and power of CozoScript, and we urge you to try Cozo for your next project—it is powerful, and fast!

## The story

You built a social network. The only thing your users could on the network do is to update their "mood". For example, user "joe" may be "moody" yesterday, and "happy" today. How is this reflected in the database? In terms of CozoScript, this can be stored in the following relation:

```
:create status {uid: String => mood: String}
```

The equivalent SQL (we will be using the Postgres dialect) would be

```sql
create table status (
    uid text primary key,
    status text not null
)
```

Your home page is very simple: it is a gigantic page containing the moods of all the users all at once. To generate it, you have the query:

```
?[uid, mood] := *status{uid, mood}
```

```sql
select uid, mood from status
```

And when users want to update their status, you just run:

```
?[uid, mood] <- $input
:put status {uid => mood}
```

```sql
update status set mood = $1 where uid = $2
```

When a user is fed up with your service and wants to quit, you run:

```
?[uid] <- $input
:rm status {uid}
```

```sql
delete from status where uid = $1
```

Simple, right? Now scientists comes to you and want your data for their study of the fluctuation of moods during the pandemic. Disregarding the fact that their ulterior motive may be much more sinister, you realize that you have thrown away perhaps the single most valuable thing about the data of your social network: the _history_ of the data.

Now you travel back in time and tell the former you to amend this mistake. "It's simple enough still", the former you said and typed:

```
:create status {uid: String, ts default now() => mood: String}
```

```sql
create table status (
    uid text not null,
    ts timestamptz not null default now(),
    mood text not null,
    primary key (uid, ts)
)
```

where `ts` stands for "timestamp", time elapsed since the UNIX epoch, for example, with a default value equal to the current timestamp if during insertion a value is not provided. Well, not good enough, as now your users can no longer quit! So after some thought:

```
:create status {uid: String, ts default now() => quitted: Bool, mood: String}
```

```sql
create table status (
    uid text not null,
    ts timestamptz not null default now(),
    quitted boolean not null,
    mood text not null,
    primary key (uid, ts)
)
```

Now you need to generate your homepage:

```
?[uid, mood] := *status{uid, mood, ts, quitted}, !quitted, ts == now()
```

```sql
select uid, mood from status
where not quitted and ts = now()
```

Disaster! The homepage remains forever blank no matter what the users do! The problem is that when you generate your homepage, you can only collect data that were inserted in the past. And for past data, the condition `ts == now()` is never true!

After a lot of fumbling, you found that the following query does the trick:

```
candidates[uid, max(ts)] := *status{uid, ts}
?[uid, mood] := candidates[uid, ts], *status{uid, ts, mood, quitted}, !quitted
```

```sql
with candidates(uid, ts) as (
    select uid, max(ts) from status
    group by uid
)
select status.uid, status.mood from status
inner join candidates on status.uid = candidates.uid and status.ts = candidates.ts
where not status.quitted
```

Let's see what this query does: ...

And when the scientists asked you about data on a particular date, you just gave them:

```
candidates[uid, max(ts)] := *status{uid, ts}, ts < $date_ts
?[uid, mood] := candidates[uid, ts], *status{uid, ts, mood, quitted}, !quitted
```

```sql
with candidates(uid, ts) as (
    select uid, max(ts) from status
    where ts < $1
    group by uid
)
select status.uid, status.mood from status
inner join candidates on status.uid = candidates.uid and status.ts = candidates.ts
where not status.quitted
```

The scientists were glad that you finally learnt to time travel. "Not bad!"

### The cost of time travel

Your social network became a runaway success! As time went on, however, you noticed performance problems, and it got worse every day.

What's happening? After all, your network caters only for the students on a certain campus, and even if everyone signed up for it, there would only be 10,000 users at most. After digging into your data, you notice that some (most?) of your users are hyperactive and tend to update their mood every five minutes during their waking hour! Even though you have only run your service for three months, some of these users have already accumulated over 10,000 mood updates (sounds familiar?).

Now look at your query for front-page generation again:

```
candidates[uid, max(ts)] := *status{uid, ts}
?[uid, mood] := candidates[uid, ts], *status{uid, ts, mood, quitted}, !quitted
```

```sql
with candidates(uid, ts) as (
    select uid, max(ts) from status
    group by uid
)
select status.uid, status.mood from status
inner join candidates on status.uid = candidates.uid and status.ts = candidates.ts
where not status.quitted
```

The key point is that you must do a complete scan of your data once to get your results. For 10,000 users and 1,000 updates each (we use the mean number of mood updates, so it's 1,000 instead of 10,000), that's 10 million rows. And next year it will become more than one billion rows, since time ticks and you are thinking of expanding your service to other campuses.

Your investor, who has no doubt became aware of your problem by now, suggest the "enterprise-y" thing: pre-computing the front page and updating it periodically instead of calculating it in real time.

### Dreamy indices

Being a hacker with a big ego, you detest all things "enterprise-y" and asked yourself: "is there anything better that can be done?" Your friend, who works in finance, suggested time series databases. "It can handle data from the _stock market_ quite well, so surely it is good enough for _your_ data!" "Just index your data by the timestamp!" Well, unlike stock market indices, your data is _sparse_: it is not collected in regular intervals for all users in unison. Are you going to materialize these implicit rows so that every time a user updates her mood, _everyone else_ also gets an automatic update with their previous mood? Your cloud provider was very welcoming of this idea and urged you to sign up for their proprietary time series database. Your investor was also kind of supportive since it would make you an instant "big data" company, but worried about whether you could secure additional funding in time to cover the costs. You, ever calm, did some back-of-envelop estimates, and gave up the idea.

Your investor still wanted to talk to you over the phone, but you became very annoyed and went to sleep, clouded in anxiety. In your dream, you came to a dark, Harry-Potteresque library, with dusty shelves neatly lined up, and on the shelves were … books for the mood of your users at different times, meticulously arranged! The _tree_ backing your database had taken physical form in your dream! You walked mechanically to the first shelf, like a robot, and started to collected the mood of every user at midnight some days back.

"Aaron, 2022-09-01, 15:31, too early."

"Aaron, 2022-09-01, 15:33, still too early."

…

"Aaron, 2022-12-24, 23:59, still too early."

"Aaron, 2022-12-25, 00:34, well it's _past_ the time we want, so the _previous_ item contains the mood." (The mood was "festive", by the way.)

"Aaron, 2022-12-25, 00:42, we don't need this anymore."

"Aaron, 2022-12-25, 00:47, we don't need this anymore."

...

"Aaron, 2022-12-27, 12:31, why are we _still_ looking at Aaron, by the way?"

...

"Bean, 2022-11-27, ..."

There were two things that irked you. First, you were going through the data in the wrong direction, so that after you had gone past the expected date, you had to go back and look at the _previous_ record. This was especially annoying since some users signed up only today, and the previous record was someone else's. Second, you were walking a _tree_, so why couldn't you jump to the next user when you knew you were done with a user?

Suddenly, the books poured out of the shelves to form a tornado, swirled all over the library, and after a while returned to the shelves. "I had to do this all over again," you gruntled and walked to the first first shelf. But something had changed: you could now directly _jump_ to the beginning, and the records were in a different order, ascending by the user, as before, but _descending_ by the timestamp:

"Aaron, 2022-12-27, 03:38, too late, let's _jump_ to the book past Aaron, 2022-12-25, 00:00."

"Aaron, 2022-12-24, 23:59. Now _this_ book contains the required mood for Aaron." "Let's now jump to the book past Aaron at the BIG BANG."

"Bean, 2022-12-24, 23:11, this is already the correct book for Bean, lucky! Now let's go past Bean at the BIG BANG." "I wonder what happened to Mr Bean since Christmas Eve?"

…

Suddenly, you woke up. You rushed to your computer and wrote down what you saw, in code.

## Hopping on the leaves of the tree

## Show me the numbers!

Stories and jokes aside, serious readers like you want objective numbers to make informed choices.

## …

When I was a functional programming fantatic, I would claim that immutability models after _mathematics_ but was averse to talking about concrete numbers. Of course I also studied mathematics at a quite high level and knew all along immutability had no more to do with mathematics than mutability.

Sombre note. Are we simply collections of data in an immutable database, operated by Laplace's daemon, perhaps? I hope not, and modern physics certainly says no, no matter how you look at it: whether it is the collapse of the wave function, or the dance at the edge of chaos. So immutability is an illusion, or a platonic fantasy that we created so that we can make sense of the world. That's OK, since we can only understand the world by projecting _our_ models onto the world. Just don't become the slaves of our own creation.

Belated happy holidays!
